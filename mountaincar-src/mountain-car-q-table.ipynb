{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]\n",
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from gymnasium import wrappers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "#from tensorflow.keras.layers import Dropout\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        action_space: int,\n",
    "        discount_factor: float,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            action_space: The number of action for the environment\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.actions = action_space\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.actions))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.training_error = []\n",
    "        \n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        greedy = random.random() > self.epsilon\n",
    "\n",
    "        # exploitation\n",
    "        if greedy:\n",
    "            obs = tuple(obs)\n",
    "            # use the train net to get the action value given a state\n",
    "            return int(np.argmax(self.q_values[obs])) \n",
    "\n",
    "        # exploration\n",
    "        else:\n",
    "             return np.random.choice(self.actions)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        \n",
    "        # convert np.ndarray to hashable object\n",
    "        obs = tuple(obs)\n",
    "        next_obs = tuple(next_obs)\n",
    "        \n",
    "\n",
    "        # get the future q_value for the current observation\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        \n",
    "        # get the difference between current q_value and next observation\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        # update the q values for the current obseervation and action\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "\n",
    "        # store the training error, the goal is to reduce it\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\" Decay epsilon value by a constant\"\"\"\n",
    "\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(env):\n",
    "    rolling_length = 500\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "    axs[0].set_title(\"Episode rewards\")\n",
    "    # compute and assign a rolling average of the data to provide a smoother graph\n",
    "    reward_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "    axs[1].set_title(\"Episode lengths\")\n",
    "    length_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "    axs[2].set_title(\"Training Error\")\n",
    "    training_error_moving_average = (\n",
    "        np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 14889/100000 [04:51<27:47, 51.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(obs)\n\u001b[1;32m     41\u001b[0m \u001b[39m# execute the action\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     44\u001b[0m \u001b[39m# update the agent\u001b[39;00m\n\u001b[1;32m     45\u001b[0m agent\u001b[39m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ML-project/.venv/lib/python3.11/site-packages/gymnasium/wrappers/record_episode_statistics.py:89\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     (\n\u001b[1;32m     84\u001b[0m         observations,\n\u001b[1;32m     85\u001b[0m         rewards,\n\u001b[1;32m     86\u001b[0m         terminations,\n\u001b[1;32m     87\u001b[0m         truncations,\n\u001b[1;32m     88\u001b[0m         infos,\n\u001b[0;32m---> 89\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     90\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m     91\u001b[0m         infos, \u001b[39mdict\u001b[39m\n\u001b[1;32m     92\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`info` dtype is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(infos)\u001b[39m}\u001b[39;00m\u001b[39m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_returns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/Desktop/Projects/ML-project/.venv/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/Projects/ML-project/.venv/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:52\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m disable_render_order_enforcing\n\u001b[0;32m---> 52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODE = 100000             # number of episode to play\n",
    "GAMMA = 0.95                # discount factor\n",
    "LR = 0.001              # q-table learning rate\n",
    "EPS_MAX = 1.0               # Initial exploration probability\n",
    "EPS_MIN = 0.1           # Final exploration probability\n",
    "DECAY = EPS_MAX / (EPISODE / 2)  # reduce the exploration probability over time\n",
    "\n",
    "\n",
    "# Start with high exploration probability\n",
    "epsilon = EPS_MAX\n",
    "\n",
    "reward_sum = 0\n",
    "win = 0\n",
    "scores = list()\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "agent = RLAgent(\n",
    "    learning_rate=LR,\n",
    "    initial_epsilon=EPS_MAX,\n",
    "    epsilon_decay=DECAY,\n",
    "    final_epsilon=EPS_MIN,\n",
    "    action_space=env.action_space.n,\n",
    "    discount_factor=GAMMA,\n",
    ")\n",
    "\n",
    "time_scores = deque(maxlen=100)\n",
    "\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=EPISODE)\n",
    "for episode in tqdm(range(EPISODE)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "\n",
    "        # get an action according to epsilon greedy policy\n",
    "        action = agent.get_action(obs)\n",
    "\n",
    "        # execute the action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "\n",
    "#print(f\"Cumulative reward: {env.return_queue}\\nEpisode lengths: {env.length_queue}\")\n",
    "\n",
    "plot_stats(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
