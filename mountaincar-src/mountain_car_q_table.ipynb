{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsogIYe7GH1f"
      },
      "outputs": [],
      "source": [
        "%pip install gymnasium[classic-control]\n",
        "%pip install tensorflow\n",
        "%pip install matplotlib\n",
        "%pip install tqdm\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "from gymnasium import wrappers\n",
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BWXo2-bMGH1h"
      },
      "outputs": [],
      "source": [
        "class RLAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        action_space: int,\n",
        "        discount_factor: float,\n",
        "\n",
        "    ):\n",
        "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
        "        of state-action values (q_values), a learning rate and an epsilon.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate\n",
        "            initial_epsilon: The initial epsilon value\n",
        "            epsilon_decay: The decay for epsilon\n",
        "            final_epsilon: The final epsilon value\n",
        "            action_space: The number of action for the environment\n",
        "            discount_factor: The discount factor for computing the Q-value\n",
        "        \"\"\"\n",
        "        self.actions = action_space\n",
        "        self.q_values = defaultdict(lambda: np.zeros(self.actions))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "        self.training_error = []\n",
        "\n",
        "\n",
        "    def get_action(self, obs: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Returns the best action with probability (1 - epsilon)\n",
        "        otherwise a random action with probability epsilon to ensure exploration.\n",
        "        \"\"\"\n",
        "        greedy = random.random() > self.epsilon\n",
        "\n",
        "        # exploitation\n",
        "        if greedy:\n",
        "            obs = tuple(obs)\n",
        "            # use the train net to get the action value given a state\n",
        "            return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "        # exploration\n",
        "        else:\n",
        "             return np.random.choice(self.actions)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "\n",
        "        # convert np.ndarray to hashable object\n",
        "        obs = tuple(obs)\n",
        "        next_obs = tuple(next_obs)\n",
        "\n",
        "\n",
        "        # get the future q_value for the current observation\n",
        "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
        "\n",
        "        # get the difference between current q_value and next observation\n",
        "        temporal_difference = (\n",
        "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
        "        )\n",
        "\n",
        "        # update the q values for the current obseervation and action\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "\n",
        "        # store the training error, the goal is to reduce it\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\" Decay epsilon value by a constant\"\"\"\n",
        "\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jB3DEx9yGH1i"
      },
      "outputs": [],
      "source": [
        "def plot_stats(env):\n",
        "    rolling_length = 500\n",
        "    fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
        "    axs[0].set_title(\"Episode rewards\")\n",
        "    # compute and assign a rolling average of the data to provide a smoother graph\n",
        "    reward_moving_average = (\n",
        "        np.convolve(\n",
        "            np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
        "        )\n",
        "        / rolling_length\n",
        "    )\n",
        "    axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
        "    axs[1].set_title(\"Episode lengths\")\n",
        "    length_moving_average = (\n",
        "        np.convolve(\n",
        "            np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
        "        )\n",
        "        / rolling_length\n",
        "    )\n",
        "    axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
        "    axs[2].set_title(\"Training Error\")\n",
        "    training_error_moving_average = (\n",
        "        np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
        "        / rolling_length\n",
        "    )\n",
        "    axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "Y5Rm1R9lGH1j",
        "outputId": "53507df4-212f-4caa-fef9-03178309c781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/500000, e 1.000000, avg reward -200.00, state [-0.5191217   0.00074906], time 200, win 0\n",
            "Episode 200/500000, e 0.818649, avg reward -200.00, state [-0.4309433  -0.00298946], time 200, win 0\n",
            "Episode 400/500000, e 0.670186, avg reward -200.00, state [-0.6146651   0.01069279], time 200, win 0\n",
            "Episode 600/500000, e 0.548647, avg reward -200.00, state [-0.67314684  0.0033727 ], time 200, win 0\n",
            "Episode 800/500000, e 0.449149, avg reward -200.00, state [-0.589977    0.00986916], time 200, win 0\n",
            "Episode 1000/500000, e 0.367695, avg reward -200.00, state [-0.5316735   0.00805567], time 200, win 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9f8c1444a658>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# get an action according to epsilon greedy policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# execute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-04dde0484d67>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# use the train net to get the action value given a state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \"\"\"\n\u001b[1;32m   1215\u001b[0m     \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPISODE = 500_000             # number of episode to play\n",
        "GAMMA = 0.99                # discount factor\n",
        "LR = 0.001251              # q-table learning rate\n",
        "EPS_MAX = 1.0               # Initial exploration probability\n",
        "EPS_MIN = 0.1           # Final exploration probability\n",
        "#DECAY = EPS_MAX / (EPISODE / 2)  # reduce the exploration probability over time\n",
        "DECAY = 0.999\n",
        "\n",
        "# Start with high exploration probability\n",
        "epsilon = EPS_MAX\n",
        "\n",
        "reward_sum = 0\n",
        "win = 0\n",
        "scores = list()\n",
        "\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "agent = RLAgent(\n",
        "    learning_rate=LR,\n",
        "    initial_epsilon=EPS_MAX,\n",
        "    epsilon_decay=DECAY,\n",
        "    final_epsilon=EPS_MIN,\n",
        "    action_space=env.action_space.n,\n",
        "    discount_factor=GAMMA,\n",
        ")\n",
        "\n",
        "reward_sum = 0\n",
        "win = 0\n",
        "time_scores = deque(maxlen=100)\n",
        "\n",
        "\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=EPISODE)\n",
        "#for episode in tqdm(range(EPISODE)):\n",
        "for episode in range(EPISODE):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    step = 1\n",
        "    # play one episode\n",
        "    while not done:\n",
        "\n",
        "        # get an action according to epsilon greedy policy\n",
        "        action = agent.get_action(obs)\n",
        "\n",
        "        # execute the action\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # update the agent\n",
        "        agent.update(obs, action, reward, terminated, next_obs)\n",
        "\n",
        "        # update if the environment is done and the current obs\n",
        "        done = terminated or truncated\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "        if done:\n",
        "\n",
        "            # store current time for that episode\n",
        "            time_scores.append(step)\n",
        "\n",
        "            # compute avg score\n",
        "            scores_avg = np.mean(time_scores) * -1\n",
        "\n",
        "            # if goal increase number of win\n",
        "            if next_obs[0] >= 0.5:\n",
        "                win += 1\n",
        "\n",
        "            if episode % 200 == 0:\n",
        "                print(f\"Episode {episode}/{EPISODE}, e {agent.epsilon:.6f}, avg reward {scores_avg:.2f}, state {next_obs}, time {step}, win {win}\")\n",
        "            break\n",
        "\n",
        "        # increment step\n",
        "        step+=1\n",
        "\n",
        "\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "# plot stats\n",
        "plot_stats(env)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}