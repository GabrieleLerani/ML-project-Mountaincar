{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ic_huuZ28F"
      },
      "outputs": [],
      "source": [
        "%pip install gymnasium[classic-control]\n",
        "%pip install tensorflow\n",
        "%pip install tdqm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import os\n",
        "import datetime\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from gymnasium import wrappers\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awctTJLMfhLW"
      },
      "source": [
        "Use GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "747K_7fOfguO",
        "outputId": "8bd75a0f-d479-4d18-d60a-d1d79a93d38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svkrLLoKZ28H"
      },
      "source": [
        "Game Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzlYKkM9Z28I",
        "outputId": "d0d3c451-9639-4749-f088-80cf089ffddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2,)\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('MountainCar-v0',render_mode=\"human\")\n",
        "print(env.observation_space.shape)\n",
        "print(len(env.observation_space.sample()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rh1hHVZ28J"
      },
      "source": [
        "Notice that in mountain car you should use truncated flag to terminate the episode, since at initial steps the flag might not be reached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp1RNqIOZ28J"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "observation, info = env.reset()\n",
        "\n",
        "episodes = 2\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # pick random action as policy\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        print(obs)\n",
        "        if obs[0] >= 0.1:\n",
        "            print(obs)\n",
        "        if truncated:\n",
        "            break\n",
        "        score += reward\n",
        "\n",
        "    print(f\"Episode {episode}, Score: {score}\")\n",
        "\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrWFiRnZ28J"
      },
      "source": [
        "We use two network of the same type, one for traininig and one which is periodically updated with new weights.Since the real target will change each time the model updates itself the solution is to create a target network that is essentially a copy of the training model at certain time steps so the target model updates less frequently\n",
        "In train function we update the target value of the train model with r if it's a terminal state, otherwise we use the Bellamn equation, this makes sense because in a terminal state, there are no more future states or actions to consider, so the only contribution to the agent's total expected reward is the immediate reward associated with reaching that terminal state.\n",
        "\n",
        "Here the metric is \"accuracy\" and the loss is \"MeanSquaredError\"\n",
        "Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "koZqhgpwZ28K"
      },
      "outputs": [],
      "source": [
        "class RLAgent:\n",
        "    def __init__(self,num_states,actions,gamma,lr) -> None:\n",
        "        self.num_states = num_states\n",
        "        self.actions = actions\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.target_net = self.build_model()\n",
        "        self.train_net = self.build_model()\n",
        "\n",
        "\n",
        "    def save_model(self,eps):\n",
        "        self.train_net.save('./trainNetwork{}.h5'.format(eps))\n",
        "\n",
        "    def load(self,eps):\n",
        "        self.train_net = load_model('./trainNetwork{}.h5'.format(eps))\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Builds a deep neural net which predicts the Q values for all possible\n",
        "        actions given a state. The input should have the shape of the state\n",
        "        (which is 2 in MountainCar), and the output should have the same shape as\n",
        "        the action space (which is 2 in MountainCar) since we want 1 Q value per\n",
        "        possible action.\n",
        "\n",
        "        :return: the Q network\n",
        "        \"\"\"\n",
        "\n",
        "        # apply learning rate EsponentialDecay\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            self.lr,\n",
        "            decay_steps=50000,\n",
        "            decay_rate=0.96,\n",
        "            staircase=True\n",
        "        )\n",
        "\n",
        "        opt = tf.optimizers.Adam(learning_rate=self.lr)\n",
        "\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=(self.num_states,), activation='relu', kernel_initializer='he_uniform'))\n",
        "        #model.add(Dropout(0.5))  # Add dropout after the first dense layer\n",
        "        #model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        #model.add(Dropout(0.08))  # Add dropout after the second dense layer\n",
        "        model.add(Dense(48, activation='relu', kernel_initializer='he_uniform'))\n",
        "        #model.add(Dropout(0.5))  # Add dropout after the third dense layer\n",
        "        model.add(Dense(self.actions, activation='linear', kernel_initializer='he_uniform'))\n",
        "        model.compile(optimizer=opt, loss='mse', metrics=[\"mse\"])\n",
        "        return model\n",
        "\n",
        "    def policy(self,state):\n",
        "        action_q = self.train_net(np.atleast_2d(state))\n",
        "        return np.argmax(action_q[0], axis=0)\n",
        "\n",
        "\n",
        "    def get_action(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        Get an action with an epsilon greedy policy\n",
        "        \"\"\"\n",
        "        greedy = random.random() > epsilon\n",
        "\n",
        "        # exploitation\n",
        "        if greedy:\n",
        "\n",
        "            # use the train net to get the action value given a state\n",
        "            return self.policy(state)\n",
        "\n",
        "        # exploration\n",
        "        else:\n",
        "             return np.random.choice(self.actions)\n",
        "\n",
        "    def train(self, batch):\n",
        "        \"\"\"\n",
        "        Train the network with a batch sample using a train net and a target net\n",
        "        \"\"\"\n",
        "\n",
        "        state, next_state, action, reward, terminated = batch\n",
        "\n",
        "        # get the current q value for that state, it will be a value for both actions\n",
        "        current_q = self.train_net(state)\n",
        "\n",
        "        # copy that value of the current q-value into a target variable\n",
        "        target_q = np.copy(current_q)\n",
        "\n",
        "        # using the target network get the q-value of the next state\n",
        "        #next_q = self.target_net(next_state)\n",
        "        next_q = self.train_net(next_state)\n",
        "\n",
        "        # among the q-values returned by the target network select the best\n",
        "        max_next_q = np.amax(next_q, axis=1)\n",
        "\n",
        "        for i in range(state.shape[0]):\n",
        "\n",
        "            target_q[i][action[i]] = reward[i] + self.gamma * (1 - terminated[i]) * max_next_q[i]\n",
        "\n",
        "\n",
        "        # fit the train model\n",
        "        result = self.train_net.fit(x=state, y=target_q, epochs=1,verbose=0)\n",
        "\n",
        "        # return the loss\n",
        "        return result.history['loss'][0]\n",
        "\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update target network with weights of train network\n",
        "        \"\"\"\n",
        "        self.target_net.set_weights(self.train_net.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NlPD8_TZ28M"
      },
      "source": [
        "Why Replay buffer?\n",
        "More Efficient Use of Samples: RL agents often experience a high sample complexity, requiring a large number of interactions with the environment to learn effectively. The replay buffer enables more efficient use of these samples by reusing them multiple times during training. It allows the agent to extract more information from each sample and learn from diverse experiences without needing to interact with the environment at every training step.\n",
        "\n",
        "Stability and Convergence: Using a replay buffer can improve the stability and convergence of RL algorithms. By breaking the correlation between consecutive samples and providing a more diverse training dataset, it reduces the variance of updates and prevents the agent from getting stuck in suboptimal states. This stability promotes better convergence and more robust learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0w3Om-bZ28M"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self,exp_max_size,batch_size):\n",
        "    self.exp_max_size = exp_max_size\n",
        "    self.batch_size = batch_size\n",
        "    self.experiences = deque(maxlen=exp_max_size)\n",
        "\n",
        "  def get_exp_size(self):\n",
        "    \"\"\"\n",
        "    Get experiences length\n",
        "    \"\"\"\n",
        "    return len(self.experiences)\n",
        "\n",
        "  def add_experience(self,exp):\n",
        "    \"\"\"\n",
        "    Add new experience to buffer\n",
        "    \"\"\"\n",
        "    # oldest item are automatically removed when dimensione is over max_exp_size\n",
        "    self.experiences.append(exp)\n",
        "\n",
        "\n",
        "  def sample_game_batch(self):\n",
        "    \"\"\"\n",
        "    Sample game batch for training loop\n",
        "    \"\"\"\n",
        "    # take a sample of batch size\n",
        "    sampled_gameplay_batch = random.sample(self.experiences, self.batch_size)\n",
        "\n",
        "    # define state, next_state, action ,reward, done\n",
        "    state_batch, next_state_batch, action_batch, reward_batch, done_batch= [], [], [], [], [],\n",
        "\n",
        "    # for each experience in the batch get a sample\n",
        "    for gameplay_experience in sampled_gameplay_batch:\n",
        "      state_batch.append(gameplay_experience[0])\n",
        "      next_state_batch.append(gameplay_experience[1])\n",
        "      reward_batch.append(gameplay_experience[2])\n",
        "      action_batch.append(gameplay_experience[3])\n",
        "      done_batch.append(gameplay_experience[4])\n",
        "\n",
        "\n",
        "    # TODO check if it works and why numpy array only for first two\n",
        "    return np.array(state_batch), np.array(next_state_batch), np.array(action_batch), np.array(reward_batch), np.array(done_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoyUp7dZZ28N"
      },
      "source": [
        "Helper function used to evaluate training process, it returns the average rewards of 10 episode. Notice that the action is taken according to the currently learned policy and not with epsilon greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pPA7s42a4uFs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_avg_rew(episode_cnt,avg_rewards):\n",
        "\n",
        "    plt.plot(range(episode_cnt), avg_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Average Reward per Episode')\n",
        "    plt.ylim(-200, None)\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(actions):\n",
        "    temp_action_x = list(actions.keys())\n",
        "\n",
        "    action_labels = {0: \"left\", 1: \"stay\", 2: \"right\"}\n",
        "    action_x = [action_labels[a] for a in temp_action_x]\n",
        "    action_y = list(actions.values())\n",
        "\n",
        "    colors = ['blue', 'green', 'orange']\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(action_x, action_y, color=colors)\n",
        "    ax.set_ylabel('Ocurrences')\n",
        "    ax.set_title('Actions')\n",
        "    ax.legend(title='Actions policy')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QbY4E1M7Z28N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "84cd4e93-7d7e-41d2-f735-829355ab204d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16/100 [00:00<00:01, 75.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [00:00<00:01, 66.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 46/100 [00:00<00:00, 64.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 72/100 [00:01<00:00, 53.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 90/100 [00:01<00:00, 50.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 56.99it/s]\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: -200.0, Accuracy 0.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def training_result():\n",
        "\n",
        "  env = gym.make('MountainCar-v0')\n",
        "\n",
        "  total_reward = 0.0\n",
        "  win = 0\n",
        "  episodes = 100\n",
        "  actions = {0:0,\n",
        "             1:0,\n",
        "             2:0}\n",
        "\n",
        "  agent = RLAgent(num_states=len(env.observation_space.sample()),actions=env.action_space.n,gamma=0.99,lr=0.001251)\n",
        "  agent.load(\"2500\")\n",
        "\n",
        "\n",
        "  for i in tqdm(range(episodes)): # Play 10 episode and take the average\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_reward = 0.0\n",
        "    while not (done or truncated):\n",
        "      #action = agent.policy(state)\n",
        "      action = np.random.choice(env.action_space.n)\n",
        "      next_state, reward, done,truncated, info = env.step(action)\n",
        "\n",
        "      # increment action\n",
        "      actions[action] += 1\n",
        "\n",
        "\n",
        "      # Count number of win\n",
        "      if next_state[0] >= 0.5:\n",
        "        win += 1\n",
        "\n",
        "      episode_reward += reward\n",
        "      state = next_state\n",
        "\n",
        "    if i % 20 == 0:\n",
        "        print(f\"{i}/{episodes}\")\n",
        "\n",
        "    total_reward += episode_reward\n",
        "\n",
        "  average_reward = total_reward / episodes\n",
        "  accuracy = win / episodes\n",
        "\n",
        "  print(f\"Average reward: {average_reward}, Accuracy {accuracy:.4f}\")\n",
        "  plot_policy(actions)\n",
        "\n",
        "\n",
        "training_result()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOwy7iuRZ28O"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNqBB7BSZ28O"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "EPISODE = 3000             # number of episode to play\n",
        "EPISODE_MAX_LENGTH = 200\n",
        "UPGRADE_STEP = 20            # frequency of target network upgrade\n",
        "SAVE_MODEL_STEP = 500        # frequency of saving model\n",
        "GAMMA = 0.99                # discount factor\n",
        "EXP_MAX_SIZE = 20_000         # Max batch size of past experience\n",
        "EXP_MIN_SIZE = 1500           # Min batch size of past experience\n",
        "LR = 0.001251               # NN learning rate\n",
        "EPS_MAX = 1.0               # Initial exploration probability\n",
        "EPS_MIN = 0.00001           # Final exploration probability\n",
        "#DECAY = 1-EPS_MAX/EPISODE  # Defines scaling factor of epsilon\n",
        "DECAY = 0.85\n",
        "BATCH_SIZE = 32            # Sample to get from experiences\n",
        "\n",
        "# Start with high exploration probability\n",
        "epsilon = EPS_MAX\n",
        "\n",
        "reward_sum = 0\n",
        "win = 0\n",
        "scores = list()\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "agent = RLAgent(num_states=len(env.observation_space.sample()),actions=env.action_space.n,gamma=GAMMA,lr=LR)\n",
        "buffer = ReplayBuffer(exp_max_size=EXP_MAX_SIZE,batch_size=BATCH_SIZE)\n",
        "time_scores = deque(maxlen=100)\n",
        "\n",
        "#agent.load(\"200\")\n",
        "\n",
        "start = time.time()\n",
        "for episode_cnt in range(EPISODE):\n",
        "  state, _ = env.reset()\n",
        "  terminated = False\n",
        "\n",
        "  # play the game and collect experience\n",
        "  for step in range(EPISODE_MAX_LENGTH):\n",
        "    action = agent.get_action(state,epsilon)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    # add experience tu the buffer\n",
        "    buffer.add_experience((state, next_state, reward, action, terminated))\n",
        "\n",
        "    # agent won't start learning if there isn't enough experience\n",
        "    if buffer.get_exp_size() > BATCH_SIZE and step % 15 == 0:\n",
        "        gameplay_experience_batch = buffer.sample_game_batch()\n",
        "        loss = agent.train(gameplay_experience_batch)\n",
        "\n",
        "    # set state to next state\n",
        "    state = next_state\n",
        "\n",
        "\n",
        "    if terminated or truncated:\n",
        "\n",
        "        # store current time for that episode\n",
        "        time_scores.append(step)\n",
        "\n",
        "        # compute avg score\n",
        "        scores_avg = np.mean(time_scores) * -1\n",
        "\n",
        "        #if episode_cnt % 250 == 0:\n",
        "            # store avg score\n",
        "        scores.append(scores_avg)\n",
        "\n",
        "        # if goal increase number of win\n",
        "        if next_state[0] >= 0.5:\n",
        "            win += 1\n",
        "\n",
        "        print(f\"Episode {episode_cnt}/{EPISODE}, e {epsilon:.6f}, exp size {buffer.get_exp_size()}, avg reward {scores_avg:.2f}, state {state}, time {step}, win {win}\")\n",
        "        break\n",
        "\n",
        "#   if buffer.get_exp_size() > BATCH_SIZE:\n",
        "#         gameplay_experience_batch = buffer.sample_game_batch()\n",
        "#         loss = agent.train(gameplay_experience_batch)\n",
        "\n",
        "\n",
        "  # update target network after one episode playing\n",
        "  #if episode_cnt % UPGRADE_STEP == 0 and episode_cnt > 0:\n",
        "  #agent.update_target_network()\n",
        "\n",
        "  # decay epsilon to increase exploitation probability\n",
        "  epsilon = max(EPS_MIN, epsilon * DECAY)\n",
        "\n",
        "  if episode_cnt % SAVE_MODEL_STEP == 0 and episode_cnt > 0:\n",
        "    agent.save_model(episode_cnt)\n",
        "\n",
        "  # display results\n",
        "  if episode_cnt % 2999 == 0 and episode_cnt > 0:\n",
        "    plot_avg_rew(episode_cnt+1,scores)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Learning duration: {end-start:.3f}\")\n",
        "\n",
        "# plot reward score\n",
        "plot_avg_rew(EPISODE,scores)\n",
        "training_result(env,agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQEnq-8YeSSY"
      },
      "source": [
        "Execution only with train net, then check execution with target net and report:\n",
        "\n",
        "\n",
        "\n",
        "*   Average reward: -137.273 after 3000 Accuracy 0.74 with % 10 no target_net no dropout\n",
        "*   Average reward: -125.276 after 3000 episodes with % 15, Accuracy 0.9910 no target_net no dropout\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}