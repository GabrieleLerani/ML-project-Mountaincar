{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CvL-8SoEO7Ki"
      },
      "source": [
        "# Install and import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ic_huuZ28F"
      },
      "outputs": [],
      "source": [
        "%pip install gymnasium[classic-control]\n",
        "%pip install tensorflow\n",
        "%pip install tdqm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdsFdB4pDvvn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import os\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from gym import wrappers\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from collections import deque"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "awctTJLMfhLW"
      },
      "source": [
        "Use GPU acceleration if available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "747K_7fOfguO",
        "outputId": "6f2d0500-a9fd-4300-9f5e-2d55aeab6513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrWFiRnZ28J"
      },
      "source": [
        "\n",
        "# RLAgent class and network implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koZqhgpwZ28K"
      },
      "outputs": [],
      "source": [
        "class RLAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        gamma: float,\n",
        "        lr: float,\n",
        "        dropout_rate: float = 0.3\n",
        "    ):\n",
        "\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.actions = env.action_space.n\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "        self.lr = lr\n",
        "        self.train_net = self.build_model()\n",
        "        self.loss_values = list()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def save_model(\n",
        "        self,\n",
        "        model_name:str,\n",
        "        episode: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Saveing model\n",
        "        \"\"\"\n",
        "        self.train_net.save(f'./{model_name}/trainNetwork{episode}.h5')\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        model_name:str,\n",
        "        episode: int\n",
        "    ):\n",
        "        self.train_net = load_model(f'./{model_name}/trainNetwork{episode}.h5')\n",
        "\n",
        "    @staticmethod\n",
        "    def get_optmizer(lr: int):\n",
        "\n",
        "        # apply learning rate EsponentialDecay\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            lr,\n",
        "            decay_steps=500,\n",
        "            decay_rate=0.96,\n",
        "            staircase=True\n",
        "        )\n",
        "\n",
        "        # define an optimizer with a learning rate schedule to decrease it over time\n",
        "        opt = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
        "        return opt\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Builds a deep neural net which predicts the Q values for all possible\n",
        "        actions given a state. The input should have the shape of the state\n",
        "        (which is 2 in MountainCar), and the output should have the same shape as\n",
        "        the action space (which is 2 in MountainCar) since we want 1 Q value per\n",
        "        possible action.\n",
        "\n",
        "        :return: the Q network\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=self.state_shape, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dropout(0.0))\n",
        "        model.add(Dense(48, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dropout(0.0))\n",
        "\n",
        "        model.add(Dense(self.actions, activation='linear',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "\n",
        "        model.compile(optimizer=self.get_optmizer(self.lr), loss='mse', metrics=[\"mse\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def update_dropout(self):\n",
        "        # Enable dropout after the specified episode\n",
        "        for layer in self.train_net.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer.rate = self.dropout_rate\n",
        "\n",
        "    def policy(self,state) -> int:\n",
        "        action_q = self.train_net(np.atleast_2d(state))\n",
        "        return np.argmax(action_q[0], axis=0)\n",
        "\n",
        "\n",
        "    def get_action(self, obs) -> int:\n",
        "        \"\"\"\n",
        "        Get an action with an epsilon greedy policy\n",
        "        \"\"\"\n",
        "        greedy = random.random() > self.epsilon\n",
        "\n",
        "        # exploitation\n",
        "        if greedy:\n",
        "\n",
        "            # use the train net to get the action value given a state\n",
        "            return self.policy(obs)\n",
        "\n",
        "        # exploration\n",
        "        else:\n",
        "             return np.random.choice(self.actions)\n",
        "\n",
        "    def train(self, batch):\n",
        "        \"\"\"\n",
        "        Train the network with a batch sample using a train net\n",
        "        \"\"\"\n",
        "\n",
        "        state, next_state, action, reward, terminated = batch\n",
        "\n",
        "        # get the current q value for that state, it will be a value for both actions\n",
        "        current_q = self.train_net(state)\n",
        "\n",
        "        # copy that value of the current q-value into a target variable\n",
        "        target_q = np.copy(current_q)\n",
        "\n",
        "        # using the train network get the q-value of the next state\n",
        "        next_q = self.train_net(next_state)\n",
        "\n",
        "        # among the q-values returned by the target network select the best\n",
        "        max_next_q = np.amax(next_q, axis=1)\n",
        "\n",
        "        for i in range(state.shape[0]):\n",
        "\n",
        "            target_q[i][action[i]] = reward[i] + self.gamma * (1 - terminated[i]) * max_next_q[i]\n",
        "\n",
        "        # fit the train model\n",
        "        history = self.train_net.fit(x=state, y=target_q, epochs=1,verbose=0)\n",
        "\n",
        "        # add to list\n",
        "        self.loss_values.append(history.history[\"loss\"])\n",
        "\n",
        "        # return the loss and learning rate\n",
        "        return round(self.train_net.optimizer.lr.numpy(), 5)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\" Decay epsilon value by a constant\"\"\"\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.plot(range(len(self.loss_values)), self.loss_values)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_rew(self,aggr_ep_rewards,model_name):\n",
        "\n",
        "        plt.plot(aggr_ep_rewards.get('ep'), aggr_ep_rewards.get('avg'), label=\"avg rewards\")\n",
        "        plt.plot(aggr_ep_rewards.get('ep'), aggr_ep_rewards.get('min'), label=\"min rewards\")\n",
        "        plt.plot(aggr_ep_rewards.get('ep'), aggr_ep_rewards.get('max'), label=\"max rewards\")\n",
        "        plt.legend(loc=4)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.ylim(-200, None)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_policy(self,actions):\n",
        "        temp_action_x = list(actions.keys())\n",
        "\n",
        "        action_labels = {0: \"left\", 1: \"stay\", 2: \"right\"}\n",
        "        action_x = [action_labels[a] for a in temp_action_x]\n",
        "        action_y = list(actions.values())\n",
        "\n",
        "        colors = ['blue', 'green', 'orange']\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.bar(action_x, action_y, color=colors)\n",
        "        ax.set_ylabel('Ocurrences')\n",
        "        ax.set_title('Actions')\n",
        "        ax.legend(title='Actions policy')\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9NlPD8_TZ28M"
      },
      "source": [
        "# Replay buffer implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0w3Om-bZ28M"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self,exp_max_size,batch_size):\n",
        "    self.exp_max_size = exp_max_size\n",
        "    self.batch_size = batch_size\n",
        "    self.experiences = deque(maxlen=exp_max_size)\n",
        "\n",
        "  def get_exp_size(self):\n",
        "    \"\"\"\n",
        "    Get experiences length\n",
        "    \"\"\"\n",
        "    return len(self.experiences)\n",
        "\n",
        "  def add_experience(self,exp):\n",
        "    \"\"\"\n",
        "    Add new experience to buffer\n",
        "    \"\"\"\n",
        "    # oldest item are automatically removed when dimensione is over max_exp_size\n",
        "    self.experiences.append(exp)\n",
        "\n",
        "\n",
        "  def sample_game_batch(self):\n",
        "    \"\"\"\n",
        "    Sample game batch for training loop\n",
        "    \"\"\"\n",
        "    # take a sample of batch size\n",
        "    sampled_gameplay_batch = random.sample(self.experiences, self.batch_size)\n",
        "\n",
        "    # define state, next_state, action ,reward, done\n",
        "    state_batch, next_state_batch, action_batch, reward_batch, done_batch= [], [], [], [], [],\n",
        "\n",
        "    # for each experience in the batch get a sample\n",
        "    for gameplay_experience in sampled_gameplay_batch:\n",
        "      state_batch.append(gameplay_experience[0])\n",
        "      next_state_batch.append(gameplay_experience[1])\n",
        "      reward_batch.append(gameplay_experience[2])\n",
        "      action_batch.append(gameplay_experience[3])\n",
        "      done_batch.append(gameplay_experience[4])\n",
        "\n",
        "    return np.array(state_batch), np.array(next_state_batch), np.array(action_batch), np.array(reward_batch), np.array(done_batch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yLJsdlgDUncl"
      },
      "source": [
        "# Helper method used to evaluate training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbY4E1M7Z28N"
      },
      "outputs": [],
      "source": [
        "def training_result(model_name,episode):\n",
        "\n",
        "  env = gym.make('MountainCar-v0')\n",
        "\n",
        "  total_reward = 0.0\n",
        "  win = 0\n",
        "  episodes = 1000\n",
        "  actions = {0:0,\n",
        "             1:0,\n",
        "             2:0}\n",
        "\n",
        "  agent = RLAgent(\n",
        "        env=env,\n",
        "        lr=0.01,\n",
        "        initial_epsilon=1.0,\n",
        "        epsilon_decay=0.99,\n",
        "        final_epsilon=0.001,\n",
        "        gamma=0.99,\n",
        "        )\n",
        "\n",
        "  agent.load(model_name,episode)\n",
        "\n",
        "\n",
        "  for i in tqdm(range(episodes)): # Play 10 episode and take the average\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_reward = 0.0\n",
        "    while not (done or truncated):\n",
        "      action = agent.policy(state)\n",
        "\n",
        "      next_state, reward, done,truncated, info = env.step(action)\n",
        "\n",
        "      # increment action\n",
        "      actions[action] += 1\n",
        "\n",
        "\n",
        "      # Count number of win\n",
        "      if next_state[0] >= 0.5:\n",
        "        win += 1\n",
        "\n",
        "      episode_reward += reward\n",
        "      state = next_state\n",
        "    print(episode_reward)\n",
        "    if i % 20 == 0:\n",
        "        print(f\"{i}/{episodes}\")\n",
        "\n",
        "    total_reward += episode_reward\n",
        "\n",
        "  average_reward = total_reward / episodes\n",
        "  accuracy = win / episodes\n",
        "\n",
        "  print(f\"Average reward: {average_reward}, Accuracy {accuracy:.4f}\")\n",
        "  agent.plot_policy(actions)\n",
        "\n",
        "model_name = \"LR: 0.01251 - GAMMA: 0.99 - EPISODES: 4000 EPSILON: 1.0\"\n",
        "episode = \"2500\"\n",
        "\n",
        "training_result(model_name, episode)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOwy7iuRZ28O"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNqBB7BSZ28O"
      },
      "outputs": [],
      "source": [
        "EPISODE = 4000                      # Number of episode to play\n",
        "EPISODE_MAX_LENGTH = 200            # This number depends on the environment\n",
        "SAVE_MODEL_STEP = EPISODE // 8      # Frequency of saving model\n",
        "DROPOUT = 750                       # Point of insertion of dropout layers\n",
        "GAMMA = 0.99                        # Discount factor\n",
        "EXP_MAX_SIZE = 10_000               # Max batch size of past experience previous 10000\n",
        "#LR = 0.001251                      # NN learning rate\n",
        "LR = 0.01251                        # Different learning rate\n",
        "EPS_MAX = 1.0                       # Initial exploration probability\n",
        "EPS_MIN = 0.001                     # Final exploration probability before: 0,00001\n",
        "DECAY = 0.85                        # Decay value\n",
        "BATCH_SIZE = 32                     # Sample to get from experiences\n",
        "PLOT = 500                          # Frequency of plotting graphs\n",
        "\n",
        "win = 0\n",
        "scores = list()\n",
        "\n",
        "model_name = f'LR: {LR} - GAMMA: {GAMMA} -' \\\n",
        "             f' EPISODES: {EPISODE}'\\\n",
        "             f' EPSILON: {EPS_MAX}'\\\n",
        "             f\" BATCH: {BATCH_SIZE}\"\n",
        "\n",
        "\n",
        "# create model directory for storing models\n",
        "if not os.path.exists(model_name):\n",
        "    os.makedirs(model_name)\n",
        "\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "agent = RLAgent(\n",
        "        env=env,\n",
        "        lr=LR,\n",
        "        initial_epsilon=EPS_MAX,\n",
        "        epsilon_decay=DECAY,\n",
        "        final_epsilon=EPS_MIN,\n",
        "        gamma=GAMMA,\n",
        "        )\n",
        "\n",
        "\n",
        "buffer = ReplayBuffer(\n",
        "                      exp_max_size=EXP_MAX_SIZE,\n",
        "                      batch_size=BATCH_SIZE\n",
        "                      )\n",
        "\n",
        "time_scores = deque(maxlen=100)\n",
        "lr_value = 0\n",
        "\n",
        "aggr_ep_rewards = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
        "\n",
        "for episode_cnt in range(1, EPISODE + 1):\n",
        "  state, _ = env.reset()\n",
        "  terminated = False\n",
        "\n",
        "  # play the game and collect experience\n",
        "  for step in range(1, EPISODE_MAX_LENGTH + 1):\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    # add experience tu the buffer\n",
        "    buffer.add_experience((state, next_state, reward, action, terminated))\n",
        "\n",
        "    # agent won't start learning if there isn't enough experience\n",
        "    if buffer.get_exp_size() > BATCH_SIZE and step % 15 == 0:\n",
        "        gameplay_experience_batch = buffer.sample_game_batch()\n",
        "        lr_value = agent.train(gameplay_experience_batch)\n",
        "\n",
        "    # set state to next state\n",
        "    state = next_state\n",
        "\n",
        "\n",
        "    if terminated or truncated:\n",
        "\n",
        "        # store current time for that episode\n",
        "        time_scores.append(step * -1)\n",
        "\n",
        "        # compute avg score of the last 100 episodes\n",
        "        avg_reward = np.mean(time_scores)\n",
        "        min_reward = min(time_scores)\n",
        "        max_reward = max(time_scores)\n",
        "        aggr_ep_rewards['ep'].append(episode_cnt)\n",
        "        aggr_ep_rewards['avg'].append(avg_reward)\n",
        "        aggr_ep_rewards['min'].append(min_reward)\n",
        "        aggr_ep_rewards['max'].append(max_reward)\n",
        "\n",
        "        # store avg score\n",
        "        scores.append(avg_reward)\n",
        "\n",
        "\n",
        "        print(f\"Episode {episode_cnt}/{EPISODE}, e {agent.epsilon:.6f}, avg reward {avg_reward:.2f}, time {step}, lr :{lr_value:.6f}\")\n",
        "        break\n",
        "\n",
        "  if episode_cnt % SAVE_MODEL_STEP == 0:\n",
        "    agent.save_model(model_name, episode_cnt)\n",
        "\n",
        "  agent.decay_epsilon()\n",
        "\n",
        "  # show avarage reward\n",
        "  if episode_cnt % PLOT == 0:\n",
        "    agent.plot_rew(aggr_ep_rewards,model_name)\n",
        "    agent.plot_loss()\n",
        "\n",
        "  if episode_cnt == DROPOUT:\n",
        "    agent.update_dropout()\n",
        "    print(\"Starting dropout\")\n",
        "\n",
        "\n",
        "#training_result(agent,model_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
